"""
Module d'analyse technique des embeddings
"""

from typing import List, Dict, Optional, Tuple
import os, json
import numpy as np
import time
from pathlib import Path
from ..core.indexing import load_index_data
from .embedding_metrics import EmbeddingQualityAnalyzer

def get_embeddings_for_analysis(doc_id: str, pipeline_name: str, data_dir: str) -> Optional[Tuple[np.ndarray, List[str]]]:
    """
    R√©cup√®re les embeddings et textes correspondants pour l'analyse technique

    Args:
        doc_id: Identifiant du document
        pipeline_name: Nom du pipeline (semantic ou azure_semantic)
        data_dir: R√©pertoire de donn√©es

    Returns:
        Tuple (embeddings, texts) ou None si non disponible
    """
    if pipeline_name not in ['semantic', 'azure_semantic']:
        return None

    try:
        # Charger les textes et m√©tadonn√©es
        texts, meta = load_index_data(doc_id, pipeline_name, data_dir)

        # Charger les embeddings
        base = f"{data_dir}/{doc_id}/{pipeline_name}"
        embeddings_path = f"{base}/embeddings.npy"

        if not os.path.exists(embeddings_path):
            print(f"‚ö†Ô∏è Embeddings non trouv√©s pour {pipeline_name}: {embeddings_path}")
            return None

        embeddings = np.load(embeddings_path)
        print(f"‚úÖ Embeddings charg√©s pour {pipeline_name}: {embeddings.shape}")

        return embeddings, texts

    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des embeddings pour {pipeline_name}: {e}")
        return None

def analyze_pipeline_embeddings(doc_id: str, data_dir: str, pipelines: List[str] = None) -> Dict:
    """
    Analyse les embeddings de tous les pipelines disponibles

    Args:
        doc_id: Identifiant du document
        data_dir: R√©pertoire de donn√©es
        pipelines: Liste des pipelines √† analyser (par d√©faut: tous les s√©mantiques)

    Returns:
        Dictionnaire des analyses par pipeline
    """
    if pipelines is None:
        pipelines = ['semantic', 'azure_semantic']

    analyzer = EmbeddingQualityAnalyzer()
    results = {}

    print(f"\nüî¨ Analyse technique des embeddings pour: {doc_id}")

    for pipeline in pipelines:
        print(f"\nüìä Analyse du pipeline: {pipeline}")

        embedding_data = get_embeddings_for_analysis(doc_id, pipeline, data_dir)
        if embedding_data is None:
            results[pipeline] = {"error": "Embeddings non disponibles"}
            continue

        embeddings, texts = embedding_data

        try:
            # Analyse de diversit√©
            diversity_metrics = analyzer.calculate_embedding_diversity(embeddings)

            # Analyse de distribution
            distribution_metrics = analyzer.analyze_embedding_distribution(embeddings)

            # Analyse de coh√©rence s√©mantique
            coherence_metrics = analyzer.measure_semantic_coherence(embeddings, texts)

            # Statistiques de base
            basic_stats = {
                "embedding_dimension": embeddings.shape[1],
                "num_chunks": embeddings.shape[0],
                "total_texts_length": sum(len(text) for text in texts),
                "avg_text_length": sum(len(text) for text in texts) / len(texts) if texts else 0
            }

            # Assembler tous les r√©sultats
            pipeline_analysis = {
                "basic_stats": basic_stats,
                "diversity": diversity_metrics,
                "distribution": distribution_metrics,
                "semantic_coherence": coherence_metrics,
                "analysis_timestamp": time.time()
            }

            results[pipeline] = pipeline_analysis

            # Afficher un r√©sum√©
            print(f"  ‚úÖ Dimension: {basic_stats['embedding_dimension']}")
            print(f"  üìä Chunks: {basic_stats['num_chunks']}")
            print(f"  üéØ Diversit√©: {diversity_metrics.get('diversity_score', 0):.3f}")
            print(f"  üß† Coh√©rence s√©mantique: {coherence_metrics.get('semantic_coherence', 0):.3f}")

        except Exception as e:
            print(f"  ‚ùå Erreur d'analyse pour {pipeline}: {e}")
            results[pipeline] = {"error": str(e)}

    return results

def export_embeddings_for_external_analysis(doc_id: str,
                                           pipeline_name: str,
                                           data_dir: str,
                                           output_dir: str = "embeddings_export") -> Optional[str]:
    """
    Exporte les embeddings et m√©tadonn√©es pour analyse externe

    Args:
        doc_id: Identifiant du document
        pipeline_name: Nom du pipeline
        data_dir: R√©pertoire de donn√©es
        output_dir: R√©pertoire de sortie

    Returns:
        Chemin du fichier export√© ou None
    """
    embedding_data = get_embeddings_for_analysis(doc_id, pipeline_name, data_dir)
    if embedding_data is None:
        return None

    embeddings, texts = embedding_data

    # Cr√©er le r√©pertoire de sortie
    export_path = Path(output_dir) / doc_id
    export_path.mkdir(parents=True, exist_ok=True)

    # Exporter les embeddings en format numpy
    embeddings_file = export_path / f"{pipeline_name}_embeddings.npy"
    np.save(embeddings_file, embeddings)

    # Exporter les textes et m√©tadonn√©es
    texts_file = export_path / f"{pipeline_name}_texts.json"
    with open(texts_file, 'w', encoding='utf-8') as f:
        json.dump(texts, f, ensure_ascii=False, indent=2)

    # Cr√©er un fichier de m√©tadonn√©es d'export
    metadata_file = export_path / f"{pipeline_name}_export_metadata.json"
    export_metadata = {
        "doc_id": doc_id,
        "pipeline_name": pipeline_name,
        "export_timestamp": time.time(),
        "embedding_shape": embeddings.shape,
        "num_texts": len(texts),
        "files": {
            "embeddings": str(embeddings_file.name),
            "texts": str(texts_file.name)
        }
    }

    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(export_metadata, f, ensure_ascii=False, indent=2)

    print(f"üíæ Embeddings export√©s vers: {export_path}")
    print(f"   - Embeddings: {embeddings_file}")
    print(f"   - Textes: {texts_file}")
    print(f"   - M√©tadonn√©es: {metadata_file}")

    return str(metadata_file)

def run_comprehensive_embedding_analysis(doc_id: str,
                                        data_dir: str = "data",
                                        export: bool = True) -> Dict:
    """
    Lance une analyse compl√®te des embeddings pour un document

    Args:
        doc_id: Identifiant du document
        data_dir: R√©pertoire de donn√©es
        export: Exporter les r√©sultats vers des fichiers

    Returns:
        Dictionnaire complet des analyses
    """
    print(f"üöÄ Lancement de l'analyse compl√®te des embeddings pour: {doc_id}")

    # 1. Analyse des m√©triques techniques
    technical_analysis = analyze_pipeline_embeddings(doc_id, data_dir)

    # 2. Exportation si demand√©e
    export_paths = {}
    if export:
        print(f"\nüíæ Exportation des embeddings...")
        for pipeline in ['semantic', 'azure_semantic']:
            export_path = export_embeddings_for_external_analysis(doc_id, pipeline, data_dir)
            if export_path:
                export_paths[pipeline] = export_path

    # 3. R√©sum√© de l'analyse
    analysis_summary = generate_analysis_summary(technical_analysis)

    comprehensive_results = {
        "doc_id": doc_id,
        "timestamp": time.time(),
        "technical_analysis": technical_analysis,
        "analysis_summary": analysis_summary,
        "export_paths": export_paths,
        "recommendations": generate_recommendations(technical_analysis)
    }

    # Afficher le r√©sum√©
    display_analysis_summary(analysis_summary)

    return comprehensive_results

def generate_analysis_summary(technical_analysis: Dict) -> Dict:
    """
    G√©n√®re un r√©sum√© de l'analyse technique
    """
    summary = {
        "pipelines_analyzed": list(technical_analysis.keys()),
        "comparison": {}
    }

    # Comparer les pipelines
    available_pipelines = [p for p in technical_analysis.keys() if "error" not in technical_analysis[p]]

    if len(available_pipelines) >= 2:
        # Comparaison entre pipelines
        comparison = {}
        for metric_category in ["diversity", "semantic_coherence"]:
            comparison[metric_category] = {}
            for pipeline in available_pipelines:
                if metric_category in technical_analysis[pipeline]:
                    if metric_category == "diversity":
                        score = technical_analysis[pipeline][metric_category].get("diversity_score", 0)
                    elif metric_category == "semantic_coherence":
                        score = technical_analysis[pipeline][metric_category].get("semantic_coherence", 0)
                    comparison[metric_category][pipeline] = score

        summary["comparison"] = comparison

    return summary

def display_analysis_summary(summary: Dict):
    """
    Affiche un r√©sum√© de l'analyse
    """
    print(f"\nüìä R√âSUM√â DE L'ANALYSE")
    print(f"=" * 50)

    pipelines = summary.get("pipelines_analyzed", [])
    print(f"üìã Pipelines analys√©s: {', '.join(pipelines)}")

    comparison = summary.get("comparison", {})
    if comparison:
        print(f"\nüîç Comparaison des performances:")

        for metric_category, scores in comparison.items():
            if scores:
                print(f"\n  {metric_category.title()}:")
                best_pipeline = max(scores, key=scores.get)
                for pipeline, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):
                    icon = "ü•á" if pipeline == best_pipeline else "üìä"
                    print(f"    {icon} {pipeline}: {score:.3f}")

def generate_recommendations(technical_analysis: Dict) -> List[str]:
    """
    G√©n√®re des recommandations bas√©es sur l'analyse technique
    """
    recommendations = []

    available_pipelines = [p for p in technical_analysis.keys() if "error" not in technical_analysis[p]]

    if not available_pipelines:
        recommendations.append("‚ùå Aucun pipeline d'embedding disponible pour l'analyse")
        return recommendations

    # Analyse de la diversit√©
    diversity_scores = {}
    coherence_scores = {}

    for pipeline in available_pipelines:
        analysis = technical_analysis[pipeline]
        if "diversity" in analysis:
            diversity_scores[pipeline] = analysis["diversity"].get("diversity_score", 0)
        if "semantic_coherence" in analysis:
            coherence_scores[pipeline] = analysis["semantic_coherence"].get("semantic_coherence", 0)

    # Recommandations bas√©es sur la diversit√©
    if diversity_scores:
        best_diversity = max(diversity_scores, key=diversity_scores.get)
        if diversity_scores[best_diversity] > 0.3:
            recommendations.append(f"‚úÖ {best_diversity} montre une bonne diversit√© d'embeddings ({diversity_scores[best_diversity]:.3f})")
        else:
            recommendations.append(f"‚ö†Ô∏è Diversit√© d'embeddings faible. Consid√©rez augmenter la vari√©t√© des chunks")

    # Recommandations bas√©es sur la coh√©rence
    if coherence_scores:
        best_coherence = max(coherence_scores, key=coherence_scores.get)
        if coherence_scores[best_coherence] > 0.5:
            recommendations.append(f"‚úÖ {best_coherence} montre une bonne coh√©rence s√©mantique ({coherence_scores[best_coherence]:.3f})")
        else:
            recommendations.append(f"‚ö†Ô∏è Coh√©rence s√©mantique faible. Le mod√®le d'embedding pourrait √™tre am√©lior√©")

    # Recommandation g√©n√©rale
    if len(available_pipelines) == 1:
        recommendations.append("üí° Consid√©rez tester plusieurs pipelines d'embedding pour comparison")

    return recommendations