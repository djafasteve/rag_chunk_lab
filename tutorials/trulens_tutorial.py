"""
TruLens Tutorial - Observabilit√© RAG en Temps R√©el
Documentation officielle: https://trulens.org/
GitHub: https://github.com/truera/trulens
"""

import os
from typing import List, Dict
import json

def install_trulens():
    """
    Installation de TruLens
    """
    print("""
üîß INSTALLATION TRULENS

# Installation basique
pip install trulens-eval

# Installation compl√®te avec toutes les int√©grations
pip install trulens-eval[providers,local]

# Pour LangChain
pip install langchain

# Pour OpenAI
pip install openai

# V√©rification
python -c "import trulens_eval; print('TruLens install√© avec succ√®s!')"
""")

def setup_trulens_basic():
    """
    Configuration de base TruLens
    """
    setup_code = '''
# 1. Import des modules TruLens
from trulens_eval import TruChain, Feedback, Select, Tru
from trulens_eval.feedback import Groundedness
from trulens_eval.feedback.provider.openai import OpenAI as TruOpenAI
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# 2. Configuration du provider de feedback
openai_provider = TruOpenAI(api_key="your-openai-key")

# 3. D√©finition des m√©triques de feedback
groundedness = Groundedness(groundedness_provider=openai_provider)

# Feedback de pertinence question-r√©ponse
f_qa_relevance = Feedback(
    openai_provider.relevance_with_cot_reasons,
    name="Answer Relevance"
).on_input_output()

# Feedback de pertinence contexte-r√©ponse
f_context_relevance = Feedback(
    openai_provider.qs_relevance_with_cot_reasons,
    name="Context Relevance"
).on_input().on(Select.RecordCalls.retrieve.rets.collect())

# Feedback de groundedness (hallucination)
f_groundedness = Feedback(
    groundedness.groundedness_measure_with_cot_reasons,
    name="Groundedness"
).on(Select.RecordCalls.retrieve.rets.collect()).on_output()

# 4. Liste des feedbacks √† utiliser
feedbacks = [f_qa_relevance, f_context_relevance, f_groundedness]
'''

    return setup_code

def integrate_trulens_with_rag_chunk_lab():
    """
    Int√©gration TruLens avec RAG Chunk Lab
    """
    integration_code = '''
# Integration avec RAG Chunk Lab
from trulens_eval import TruChain, Feedback, Tru
import json

class TruLensRAGEvaluator:
    def __init__(self, openai_api_key: str):
        self.tru = Tru()
        self.openai_provider = TruOpenAI(api_key=openai_api_key)
        self.feedbacks = self._setup_feedbacks()

    def _setup_feedbacks(self):
        """Configure les m√©triques TruLens"""
        return [
            # Pertinence de la r√©ponse
            Feedback(
                self.openai_provider.relevance_with_cot_reasons,
                name="Answer Relevance"
            ).on_input_output(),

            # Groundedness (anti-hallucination)
            Feedback(
                self.openai_provider.groundedness_measure_with_cot_reasons,
                name="Groundedness"
            ).on_input().on_output(),

            # Coh√©rence contextuelle
            Feedback(
                self.openai_provider.context_relevance_with_cot_reasons,
                name="Context Relevance"
            ).on_input()
        ]

    def evaluate_pipeline(self, doc_id: str, pipeline_name: str,
                         questions: List[str], answers: List[str],
                         contexts: List[List[str]]) -> Dict:
        """√âvalue un pipeline avec TruLens"""

        # Simuler une chain LangChain pour TruLens
        class MockRAGChain:
            def __init__(self, answers, contexts):
                self.answers = answers
                self.contexts = contexts
                self.current_idx = 0

            def __call__(self, inputs):
                question = inputs.get("query", "")
                if self.current_idx < len(self.answers):
                    answer = self.answers[self.current_idx]
                    context = self.contexts[self.current_idx] if self.current_idx < len(self.contexts) else []
                    self.current_idx += 1
                    return {"result": answer, "source_documents": context}
                return {"result": "", "source_documents": []}

        # Cr√©er la chain mock√©e
        mock_chain = MockRAGChain(answers, contexts)

        # Wrapper TruLens
        tru_chain = TruChain(
            mock_chain,
            app_id=f"{doc_id}_{pipeline_name}",
            feedbacks=self.feedbacks
        )

        # √âvaluer chaque question
        records = []
        for question in questions:
            with tru_chain as recording:
                result = mock_chain({"query": question})
                records.append(recording.get())

        # R√©cup√©rer les r√©sultats
        results = {
            "pipeline": pipeline_name,
            "total_questions": len(questions),
            "feedback_scores": {},
            "records": records
        }

        # Calculer les scores moyens
        for feedback in self.feedbacks:
            feedback_name = feedback.name
            scores = [record.feedback_results.get(feedback_name, 0) for record in records]
            results["feedback_scores"][feedback_name] = {
                "mean": sum(scores) / len(scores) if scores else 0,
                "scores": scores
            }

        return results

    def generate_trulens_report(self, results: Dict, output_dir: str = "trulens_results"):
        """G√©n√®re un rapport TruLens"""
        os.makedirs(output_dir, exist_ok=True)

        report_file = f"{output_dir}/trulens_evaluation_{results['pipeline']}.json"
        with open(report_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)

        print(f"üìä TruLens report saved: {report_file}")

        # Lancer le dashboard TruLens
        print("üåê Starting TruLens dashboard...")
        print("Visit: http://localhost:8501")
        # self.tru.run_dashboard()  # D√©commentez pour lancer le dashboard

        return report_file

# Utilisation dans RAG Chunk Lab
def run_trulens_evaluation(doc_id: str,
                          per_pipeline_answers: Dict[str, List[str]],
                          per_pipeline_contexts: Dict[str, List[List[str]]],
                          questions: List[str],
                          openai_api_key: str) -> Dict:
    """
    Lance l'√©valuation TruLens pour tous les pipelines
    """
    evaluator = TruLensRAGEvaluator(openai_api_key)
    results = {}

    for pipeline, answers in per_pipeline_answers.items():
        contexts = per_pipeline_contexts.get(pipeline, [])

        print(f"üîç TruLens evaluation for {pipeline}...")

        pipeline_results = evaluator.evaluate_pipeline(
            doc_id, pipeline, questions, answers, contexts
        )

        results[pipeline] = pipeline_results

        # G√©n√©rer rapport
        evaluator.generate_trulens_report(pipeline_results)

        # Afficher r√©sultats
        feedback_scores = pipeline_results["feedback_scores"]
        print(f"  üìä Answer Relevance: {feedback_scores.get('Answer Relevance', {}).get('mean', 0):.3f}")
        print(f"  üéØ Groundedness: {feedback_scores.get('Groundedness', {}).get('mean', 0):.3f}")
        print(f"  üìù Context Relevance: {feedback_scores.get('Context Relevance', {}).get('mean', 0):.3f}")

    return results
'''

    return integration_code

def trulens_advanced_features():
    """
    Fonctionnalit√©s avanc√©es de TruLens
    """
    advanced_code = '''
# FONCTIONNALIT√âS AVANC√âES TRULENS

# 1. Custom Feedback Functions
from trulens_eval.feedback import Feedback

def custom_domain_relevance(question: str, answer: str) -> float:
    """Feedback personnalis√© pour votre domaine"""
    # Logique sp√©cifique √† votre domaine
    domain_keywords = ["votre", "domaine", "sp√©cifique"]

    score = 0.0
    for keyword in domain_keywords:
        if keyword in question.lower() and keyword in answer.lower():
            score += 0.2

    return min(1.0, score)

# Cr√©er le feedback personnalis√©
custom_feedback = Feedback(
    custom_domain_relevance,
    name="Domain Relevance"
).on_input_output()

# 2. Monitoring en Temps R√©el
class RealTimeMonitor:
    def __init__(self):
        self.tru = Tru()

    def setup_alerts(self):
        """Configure des alertes automatiques"""
        # D√©finir des seuils
        thresholds = {
            "groundedness": 0.7,
            "relevance": 0.8,
            "custom_metric": 0.6
        }

        # Surveillance continue
        for app_id in self.tru.get_app_ids():
            records = self.tru.get_records_and_feedback(app_ids=[app_id])

            for record in records:
                for metric, threshold in thresholds.items():
                    score = record.feedback_results.get(metric, 0)
                    if score < threshold:
                        self.send_alert(app_id, metric, score, threshold)

    def send_alert(self, app_id: str, metric: str, score: float, threshold: float):
        """Envoie une alerte"""
        message = f"üö® Alert: {app_id} - {metric}: {score:.3f} < {threshold}"
        print(message)
        # Ici vous pourriez envoyer un email, Slack, etc.

# 3. Comparaison A/B Testing
def compare_pipelines_trulens(results_a: Dict, results_b: Dict, pipeline_a: str, pipeline_b: str):
    """Compare deux pipelines avec TruLens"""

    comparison = {
        "pipeline_a": pipeline_a,
        "pipeline_b": pipeline_b,
        "winner": {},
        "detailed_comparison": {}
    }

    # Comparer chaque m√©trique
    for metric in ["Answer Relevance", "Groundedness", "Context Relevance"]:
        score_a = results_a.get("feedback_scores", {}).get(metric, {}).get("mean", 0)
        score_b = results_b.get("feedback_scores", {}).get(metric, {}).get("mean", 0)

        comparison["detailed_comparison"][metric] = {
            pipeline_a: score_a,
            pipeline_b: score_b,
            "difference": score_b - score_a,
            "winner": pipeline_a if score_a > score_b else pipeline_b
        }

        comparison["winner"][metric] = pipeline_a if score_a > score_b else pipeline_b

    # Gagnant global
    scores_a = [comp[pipeline_a] for comp in comparison["detailed_comparison"].values()]
    scores_b = [comp[pipeline_b] for comp in comparison["detailed_comparison"].values()]

    avg_a = sum(scores_a) / len(scores_a)
    avg_b = sum(scores_b) / len(scores_b)

    comparison["overall_winner"] = pipeline_a if avg_a > avg_b else pipeline_b
    comparison["overall_scores"] = {pipeline_a: avg_a, pipeline_b: avg_b}

    return comparison

# 4. Export pour Analyse Externe
def export_trulens_data(app_id: str, output_file: str):
    """Exporte les donn√©es TruLens pour analyse externe"""
    tru = Tru()

    # R√©cup√©rer toutes les donn√©es
    records = tru.get_records_and_feedback(app_ids=[app_id])

    # Pr√©parer pour export
    export_data = []
    for record in records:
        export_data.append({
            "timestamp": record.timestamp,
            "input": record.input,
            "output": record.output,
            "feedback_scores": record.feedback_results,
            "latency": record.latency,
            "cost": record.cost
        })

    # Sauvegarder
    with open(output_file, 'w') as f:
        json.dump(export_data, f, indent=2, default=str)

    print(f"üìÅ TruLens data exported to: {output_file}")
'''

    return advanced_code

def trulens_dashboard_guide():
    """
    Guide d'utilisation du dashboard TruLens
    """
    guide = '''
# üåê GUIDE DASHBOARD TRULENS

## Lancement du Dashboard
```python
from trulens_eval import Tru

tru = Tru()
tru.run_dashboard(port=8501, host="localhost")
```

## Navigation du Dashboard

### 1. üìä Overview Tab
- Vue d'ensemble de toutes vos applications
- Scores moyens par application
- Tendances temporelles
- Comparaison rapide des performances

### 2. üîç Applications Tab
- D√©tails par application/pipeline
- Historique des √©valuations
- Breakdown par m√©trique
- Records individuels

### 3. üìà Records Tab
- Vue d√©taill√©e de chaque interaction
- Input/Output avec scores
- Cha√Æne de raisonnement (Chain of Thought)
- Temps de r√©ponse et co√ªts

### 4. ‚öôÔ∏è Feedback Tab
- Configuration des m√©triques
- Fonctions de feedback personnalis√©es
- Providers de feedback (OpenAI, Hugging Face, etc.)

## Fonctionnalit√©s Cl√©s

### Filtrage et Recherche
- Filtrer par date, score, application
- Recherche dans les inputs/outputs
- Export des donn√©es filtr√©es

### Alertes et Monitoring
- Seuils configurables
- Notifications en temps r√©el
- Graphiques de tendances

### Comparaison A/B
- Comparer plusieurs versions
- Tests statistiques
- Visualisations interactives

## Int√©gration avec votre Workflow

```python
# 1. Dans votre script d'√©valuation
if __name__ == "__main__":
    # Lancer vos √©valuations
    run_evaluations()

    # D√©marrer le dashboard pour analyser
    tru = Tru()
    tru.run_dashboard()
    print("Dashboard accessible sur: http://localhost:8501")

# 2. En mode production
# Utilisez tru.start_dashboard_session() pour contr√¥le programmatique
```

## Tips d'Utilisation

### üìä Analyse des Performances
1. Utilisez l'onglet "Overview" pour identifier les probl√®mes
2. Drill-down dans "Records" pour les d√©tails
3. Comparez diff√©rentes configurations dans "Applications"

### üîç Debug des Probl√®mes
1. Filtrez les scores faibles
2. Examinez les cha√Ænes de raisonnement
3. Analysez les patterns dans les √©checs

### üìà Optimisation Continue
1. Suivez les tendances temporelles
2. Testez diff√©rentes configurations
3. Mesurez l'impact des changements
'''

    return guide

def trulens_best_practices():
    """
    Meilleures pratiques TruLens
    """
    practices = '''
# üèÜ MEILLEURES PRATIQUES TRULENS

## 1. Configuration Initiale
```python
# Toujours configurer des app_id explicites
tru_chain = TruChain(
    chain,
    app_id="rag_chunk_lab_semantic_v1",  # Versioning important
    feedbacks=feedbacks,
    metadata={"version": "1.0", "domain": "legal"}  # M√©tadonn√©es utiles
)
```

## 2. Choix des M√©triques
- **Answer Relevance**: Essentiel pour tout RAG
- **Groundedness**: Critique pour √©viter hallucinations
- **Context Relevance**: Important pour optimiser la r√©cup√©ration
- **Custom Metrics**: Ajoutez des m√©triques sp√©cifiques √† votre domaine

## 3. Monitoring Production
```python
# Configuration pour production
class ProductionMonitor:
    def __init__(self):
        self.tru = Tru()
        self.alert_thresholds = {
            "groundedness": 0.7,
            "answer_relevance": 0.8
        }

    def monitor_continuously(self):
        # V√©rification p√©riodique
        for app_id in self.tru.get_app_ids():
            recent_records = self.tru.get_records_and_feedback(
                app_ids=[app_id],
                limit=100  # Derniers 100 records
            )

            # Analyser les tendances
            self.analyze_trends(recent_records)
```

## 4. Optimisation des Co√ªts
- Utilisez des √©chantillons repr√©sentatifs pour l'√©valuation
- Configurez des caches pour √©viter les re-√©valuations
- Alternez entre √©valuations compl√®tes et partielles

## 5. Int√©gration CI/CD
```python
# Dans votre pipeline CI/CD
def evaluate_before_deployment():
    results = run_trulens_evaluation()

    # V√©rifier les seuils de qualit√©
    for metric, threshold in QUALITY_GATES.items():
        if results[metric] < threshold:
            raise Exception(f"Quality gate failed: {metric} = {results[metric]}")

    print("‚úÖ Quality gates passed - deployment approved")
```

## 6. Documentation et Tra√ßabilit√©
- Documentez vos m√©triques personnalis√©es
- Gardez un historique des configurations
- Versionnez vos applications TruLens
- Exportez r√©guli√®rement pour backup
'''

    return practices

# Fonction principale pour cr√©er le tutoriel complet
def create_trulens_complete_tutorial():
    """
    Cr√©e le tutoriel complet TruLens
    """
    tutorial_content = f"""
# üîç TruLens - Tutoriel Complet pour RAG Chunk Lab

{install_trulens()}

## üöÄ Configuration de Base
{setup_trulens_basic()}

## üîå Int√©gration avec RAG Chunk Lab
{integrate_trulens_with_rag_chunk_lab()}

## üéØ Fonctionnalit√©s Avanc√©es
{trulens_advanced_features()}

## üåê Guide Dashboard
{trulens_dashboard_guide()}

## üèÜ Meilleures Pratiques
{trulens_best_practices()}

## üìö Ressources Additionnelles

### Documentation Officielle
- Site web: https://trulens.org/
- Documentation: https://trulens.org/trulens_eval/
- GitHub: https://github.com/truera/trulens
- Exemples: https://github.com/truera/trulens/tree/main/trulens_eval/examples

### Tutoriels Vid√©o
- Getting Started: https://www.youtube.com/watch?v=xyz
- Advanced Features: https://www.youtube.com/watch?v=abc

### Community
- Discord: https://discord.gg/trulens
- GitHub Discussions: https://github.com/truera/trulens/discussions
"""

    # Sauvegarder le tutoriel
    with open("tutorials/trulens_complete_tutorial.md", "w", encoding="utf-8") as f:
        f.write(tutorial_content)

    return "tutorials/trulens_complete_tutorial.md"

if __name__ == "__main__":
    tutorial_file = create_trulens_complete_tutorial()
    print(f"üìö Tutoriel TruLens cr√©√©: {tutorial_file}")