#!/usr/bin/env python3
"""
Script pour exÃ©cuter tous les tests RAG Chunk Lab avec des rapports dÃ©taillÃ©s
"""
import unittest
import sys
import os
import time
from io import StringIO

# Ajouter le dossier parent au path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class ColoredTextTestResult(unittest.TextTestResult):
    """RÃ©sultats de test avec coloration pour une meilleure lisibilitÃ©"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.success_count = 0

    def addSuccess(self, test):
        super().addSuccess(test)
        self.success_count += 1
        if self.showAll:
            self.stream.write("\033[92mâœ“ PASS\033[0m\n")
        elif self.dots:
            self.stream.write("\033[92m.\033[0m")

    def addError(self, test, err):
        super().addError(test, err)
        if self.showAll:
            self.stream.write("\033[91mâœ— ERROR\033[0m\n")
        elif self.dots:
            self.stream.write("\033[91mE\033[0m")

    def addFailure(self, test, err):
        super().addFailure(test, err)
        if self.showAll:
            self.stream.write("\033[91mâœ— FAIL\033[0m\n")
        elif self.dots:
            self.stream.write("\033[91mF\033[0m")

    def addSkip(self, test, reason):
        super().addSkip(test, reason)
        if self.showAll:
            self.stream.write(f"\033[93m- SKIP ({reason})\033[0m\n")
        elif self.dots:
            self.stream.write("\033[93ms\033[0m")


class ColoredTextTestRunner(unittest.TextTestRunner):
    """Runner de test avec coloration"""
    resultclass = ColoredTextTestResult


def run_test_suite(test_pattern=None, verbosity=2):
    """ExÃ©cute la suite de tests avec rapport dÃ©taillÃ©"""

    print("\033[96m" + "="*80)
    print("ðŸ§ª RAG CHUNK LAB v2.0 - SUITE DE TESTS OPTIMISÃ‰S")
    print("="*80 + "\033[0m")

    # DÃ©couvrir tous les tests
    test_dir = os.path.dirname(os.path.abspath(__file__))
    if test_pattern:
        pattern = f"test_{test_pattern}.py"
    else:
        pattern = "test_*.py"

    loader = unittest.TestLoader()
    suite = loader.discover(test_dir, pattern=pattern)

    # Compter les tests
    test_count = suite.countTestCases()
    print(f"\nðŸ“Š Tests dÃ©couverts: {test_count}")

    # ExÃ©cuter les tests
    print("\nðŸš€ ExÃ©cution des tests...\n")

    start_time = time.time()
    runner = ColoredTextTestRunner(verbosity=verbosity, buffer=True)
    result = runner.run(suite)
    execution_time = time.time() - start_time

    # Rapport final
    print("\n" + "\033[96m" + "="*80)
    print("ðŸ“‹ RAPPORT FINAL")
    print("="*80 + "\033[0m")

    print(f"â±ï¸  Temps d'exÃ©cution: {execution_time:.2f} secondes")
    print(f"ðŸ“Š Tests exÃ©cutÃ©s: {result.testsRun}")
    print(f"\033[92mâœ“ SuccÃ¨s: {result.success_count}\033[0m")

    if result.failures:
        print(f"\033[91mâœ— Ã‰checs: {len(result.failures)}\033[0m")
    if result.errors:
        print(f"\033[91mâš  Erreurs: {len(result.errors)}\033[0m")
    if result.skipped:
        print(f"\033[93m- IgnorÃ©s: {len(result.skipped)}\033[0m")

    # Taux de rÃ©ussite
    if result.testsRun > 0:
        success_rate = (result.success_count / result.testsRun) * 100
        if success_rate == 100:
            print(f"\n\033[92mðŸŽ‰ Taux de rÃ©ussite: {success_rate:.1f}% - PARFAIT!\033[0m")
        elif success_rate >= 90:
            print(f"\n\033[92mâœ… Taux de rÃ©ussite: {success_rate:.1f}% - EXCELLENT\033[0m")
        elif success_rate >= 75:
            print(f"\n\033[93mâš ï¸  Taux de rÃ©ussite: {success_rate:.1f}% - Ã€ AMÃ‰LIORER\033[0m")
        else:
            print(f"\n\033[91mâŒ Taux de rÃ©ussite: {success_rate:.1f}% - PROBLÃˆMES DÃ‰TECTÃ‰S\033[0m")

    # DÃ©tails des Ã©checs
    if result.failures or result.errors:
        print(f"\n\033[91mðŸ“‹ DÃ‰TAILS DES PROBLÃˆMES:\033[0m")

        for test, traceback in result.failures:
            print(f"\n\033[91mâŒ Ã‰CHEC: {test}\033[0m")
            print(f"   {traceback.split('AssertionError:')[-1].strip()}")

        for test, traceback in result.errors:
            print(f"\n\033[91mâš ï¸  ERREUR: {test}\033[0m")
            print(f"   {traceback.split('Error:')[-1].strip()}")

    print("\n" + "\033[96m" + "="*80 + "\033[0m")

    return result.wasSuccessful()


def run_specific_test_category():
    """Interface pour exÃ©cuter des catÃ©gories spÃ©cifiques de tests"""

    categories = {
        '1': ('optimizations', 'Tests des optimisations gÃ©nÃ©rales'),
        '2': ('chunkers', 'Tests du module chunkers'),
        '3': ('indexing', 'Tests du module indexing'),
        '4': ('generation', 'Tests du module generation'),
        '5': ('performance', 'Tests de performance'),
        '6': (None, 'Tous les tests')
    }

    print("\nðŸŽ¯ Choisissez une catÃ©gorie de tests:")
    for key, (pattern, description) in categories.items():
        print(f"  {key}. {description}")

    choice = input("\nVotre choix (1-6): ").strip()

    if choice in categories:
        pattern, description = categories[choice]
        print(f"\nðŸš€ ExÃ©cution: {description}")
        return run_test_suite(pattern)
    else:
        print("âŒ Choix invalide")
        return False


if __name__ == '__main__':
    if len(sys.argv) > 1:
        # Mode ligne de commande
        test_pattern = sys.argv[1] if sys.argv[1] != 'all' else None
        success = run_test_suite(test_pattern)
    else:
        # Mode interactif
        success = run_specific_test_category()

    sys.exit(0 if success else 1)