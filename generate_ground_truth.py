#!/usr/bin/env python3
"""
Script standalone pour gÃ©nÃ©rer un dataset de ground truth
Utilisation: python generate_ground_truth.py --folder documents/code_penal
"""

import argparse
import sys
from pathlib import Path

# Ajouter le module au chemin
sys.path.insert(0, str(Path(__file__).parent))

from rag_chunk_lab.ground_truth_generator import GroundTruthGenerator, create_llm_client


def main():
    parser = argparse.ArgumentParser(
        description='GÃ©nÃ¨re automatiquement un dataset de ground truth Ã  partir de documents'
    )

    parser.add_argument('--folder', required=True,
                       help='Dossier contenant les documents Ã  traiter')
    parser.add_argument('--output',
                       help='Fichier JSONL de sortie (dÃ©faut: {nom_dossier}_ground_truth.jsonl)')
    parser.add_argument('--llm-provider', default='ollama', choices=['ollama', 'azure'],
                       help='Provider LLM: ollama ou azure (dÃ©faut: ollama)')
    parser.add_argument('--model',
                       help='ModÃ¨le Ã  utiliser (dÃ©faut: mistral:7b pour Ollama)')
    parser.add_argument('--questions-per-doc', type=int, default=10,
                       help='Nombre de questions Ã  gÃ©nÃ©rer pour CHAQUE document supportÃ© (dÃ©faut: 10)')
    parser.add_argument('--min-length', type=int, default=200,
                       help='Longueur minimale du texte source (dÃ©faut: 200)')
    parser.add_argument('--max-length', type=int, default=3000,
                       help='Longueur maximale du texte source (dÃ©faut: 3000)')
    parser.add_argument('--ollama-url', default='http://localhost:11434',
                       help='URL du serveur Ollama (dÃ©faut: http://localhost:11434)')
    parser.add_argument('--language', default='fr', choices=['fr', 'en', 'es'],
                       help='Langue pour la gÃ©nÃ©ration des questions (dÃ©faut: fr)')
    parser.add_argument('--question-style', default='standard', choices=['standard', 'minimal-keywords'],
                       help='Style de questions: standard (avec mots-clÃ©s) ou minimal-keywords (sans indices) (dÃ©faut: standard)')
    parser.add_argument('--allow-reuse', action='store_true',
                       help='Permettre de rÃ©utiliser les chunks pour gÃ©nÃ©rer plus de questions que de chunks disponibles')

    args = parser.parse_args()

    folder_path = Path(args.folder)

    # DÃ©finir le fichier de sortie
    if not args.output:
        args.output = f"{folder_path.name}_ground_truth.jsonl"

    print(f"ğŸš€ DÃ©marrage de la gÃ©nÃ©ration de ground truth")
    print(f"ğŸ“ Dossier source: {args.folder}")
    print(f"ğŸ¤– LLM Provider: {args.llm_provider}")
    print(f"ğŸ“Š Questions par document supportÃ©: {args.questions_per_doc}")
    print(f"ğŸ’¾ Fichier de sortie: {args.output}")

    try:
        # CrÃ©er le client LLM
        print(f"\nğŸ”Œ Connexion au LLM...")

        if args.llm_provider == 'ollama':
            llm_client = create_llm_client('ollama', args.model, base_url=args.ollama_url)
            print(f"âœ… ConnectÃ© Ã  Ollama: {llm_client.model}")
        elif args.llm_provider == 'azure':
            import os
            # VÃ©rifier les variables d'environnement
            required_vars = ['AZURE_OPENAI_API_KEY', 'AZURE_OPENAI_ENDPOINT']
            missing_vars = [var for var in required_vars if not os.getenv(var)]
            if missing_vars:
                print(f"âŒ Variables d'environnement manquantes: {', '.join(missing_vars)}")
                return 1

            llm_client = create_llm_client('azure', args.model)
            print(f"âœ… ConnectÃ© Ã  Azure OpenAI: {llm_client.model}")

        # CrÃ©er le gÃ©nÃ©rateur
        generator = GroundTruthGenerator(llm_client, language=args.language, question_style=args.question_style)

        # GÃ©nÃ©rer le dataset
        output_path = generator.generate_from_folder(
            folder_path=args.folder,
            output_path=args.output,
            questions_per_doc=args.questions_per_doc,
            min_text_length=args.min_length,
            max_text_length=args.max_length,
            allow_reuse=args.allow_reuse
        )

        print(f"\nğŸ‰ GÃ©nÃ©ration terminÃ©e avec succÃ¨s!")
        print(f"ğŸ“„ Dataset sauvegardÃ©: {output_path}")
        print(f"\nğŸ’¡ Utilisez maintenant:")
        print(f"   python -m rag_chunk_lab.cli evaluate --doc-id <votre-doc> --ground-truth {output_path} --ragas --use-llm")

        return 0

    except ConnectionError as e:
        print(f"âŒ Erreur de connexion LLM: {e}")
        if args.llm_provider == 'ollama':
            print("ğŸ’¡ Assurez-vous qu'Ollama est dÃ©marrÃ©: ollama serve")
            print(f"ğŸ’¡ Et que le modÃ¨le est installÃ©: ollama pull {args.model or 'mistral:7b'}")
        return 1
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())