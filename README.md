# RAG Chunk Lab â€” Laboratoire de Chunking pour RAG

Un outil complet pour **tester et comparer diffÃ©rentes stratÃ©gies de chunking** dans vos pipelines RAG. IdÃ©al pour optimiser la performance sur des documents juridiques, techniques ou rÃ©glementaires.

## ğŸ¯ Qu'est-ce que Ã§a fait ?

**RAG Chunk Lab** teste automatiquement **3 stratÃ©gies de chunking** sur vos documents :

1. **ğŸ“„ Fixed Chunks** â†’ DÃ©coupage fixe (500 tokens + overlap 80)
2. **ğŸ—‚ï¸ Structure-Aware** â†’ Respecte les titres et sections (Article, Chapitre...)
3. **ğŸ”„ Sliding Window** â†’ FenÃªtres glissantes (400 tokens, stride 200)

Pour chaque question, vous obtenez :
- âœ… Une rÃ©ponse par stratÃ©gie (extractive ou LLM)
- ğŸ“ Les sources exactes (page, section, snippet)
- ğŸ“Š Export CSV pour analyse dans Excel
- ğŸ¤– Ã‰valuation automatique avec mÃ©triques RAGAS

---

## ğŸš€ Installation et Premier Test

### Ã‰tape 1 : Environnement
```bash
git clone <votre-repo>
cd rag_chunk_lab
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### Ã‰tape 2 : Tester avec un document
```bash
# IngÃ©rer un document PDF/TXT/MD
python -m rag_chunk_lab.cli ingest --doc mon_document.pdf --doc-id test

# Poser une question et voir les 3 rÃ©ponses
python -m rag_chunk_lab.cli ask --doc-id test --question "Quel est le dÃ©lai de prescription ?"
```

**RÃ©sultat** : 3 rÃ©ponses comparÃ©es + fichier `exports/test/sources_<timestamp>.csv`

---

## ğŸ¤– CrÃ©er un Dataset de Test Automatiquement

### Pourquoi ?
Au lieu de crÃ©er manuellement des questions/rÃ©ponses, **gÃ©nÃ©rez automatiquement un dataset d'expert** Ã  partir de vos documents !

### Comment faire ?

#### Option A : Avec Ollama (Local, Gratuit)
```bash
# 1. Installer et dÃ©marrer Ollama
ollama serve
ollama pull mistral:7b

# 2. GÃ©nÃ©rer 10 questions expertes par document
python3 generate_ground_truth.py --folder documents/mes_docs --questions-per-doc 10
```

#### Option B : Avec Azure OpenAI (Plus performant)
```bash
# 1. Configurer Azure OpenAI
export AZURE_OPENAI_API_KEY="votre-clÃ©"
export AZURE_OPENAI_ENDPOINT="https://votre-resource.openai.azure.com"
export AZURE_OPENAI_DEPLOYMENT="gpt-4o-mini"
export AZURE_OPENAI_API_VERSION="2024-02-15-preview"

# 2. GÃ©nÃ©rer le dataset
python3 generate_ground_truth.py --folder documents/mes_docs --llm-provider azure
```

### RÃ©sultat
Fichier `mes_docs_ground_truth.jsonl` avec format :
```json
{
  "question": "Quel est le dÃ©lai de prescription pour les contraventions de 5Ã¨me classe ?",
  "answer": "Le dÃ©lai de prescription de l'action publique des contraventions de la cinquiÃ¨me classe est de trois ans rÃ©volus...",
  "source_document": "code_penal.pdf",
  "page": 15,
  "doc_section": "TITRE PREMIER",
  "generated_by": "ollama:mistral:7b"
}
```

---

## ğŸ“Š Ã‰valuation et Comparaison des StratÃ©gies

### Ã‰valuation Automatique avec RAGAS

Une fois votre dataset crÃ©Ã©, comparez les 3 stratÃ©gies de chunking :

```bash
# Ã‰valuer les 3 pipelines avec mÃ©triques d'expert
python -m rag_chunk_lab.cli evaluate \
  --doc-id test \
  --ground-truth mes_docs_ground_truth.jsonl \
  --ragas \
  --use-llm
```

**Pendant l'exÃ©cution, vous verrez :**
```
ğŸ”„ Collecting answers from 3 pipelines for 10 questions...
ğŸ“Š Processing pipelines: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:30<00:00]
âœ… Answer collection completed!

ğŸ¯ Starting RAGAS evaluation...
ğŸ”„ Starting RAGAS evaluation for 3 pipelines with 10 questions...
ğŸ“Š Metrics: answer_relevancy, faithfulness, context_precision, context_recall

ğŸ” Evaluating fixed pipeline...
  ğŸ“ Running RAGAS metrics for 'fixed' pipeline...
  âœ… fixed: answer_relevancy: 0.847, faithfulness: 0.923

âœ¨ RAGAS evaluation completed!
ğŸ“‹ Summary:
  fixed: avg=0.856
  structure: avg=0.891  â† Meilleur !
  sliding: avg=0.834

ğŸ’¾ Exporting results...
```

### Fichiers d'Analyse GÃ©nÃ©rÃ©s

AprÃ¨s Ã©valuation, vous trouvez dans `exports/test/` :

1. **`ragas_summary.csv`** â†’ Tableau de bord des moyennes
   ```
   pipeline,answer_relevancy,faithfulness,context_precision,context_recall
   fixed,0.847,0.923,0.756,0.834
   structure,0.891,0.945,0.812,0.867  â† La structure-aware gagne !
   sliding,0.834,0.898,0.743,0.801
   ```

2. **`ragas_per_question.csv`** â†’ DÃ©tail par question
   - Parfait pour identifier les questions problÃ©matiques
   - IdÃ©al pour tableaux croisÃ©s dynamiques Excel

### Analyse dans Excel

1. **Ouvrir `ragas_summary.csv`** â†’ Vue d'ensemble rapide
2. **Ouvrir `ragas_per_question.csv`** â†’ Analyser les dÃ©tails
3. **CrÃ©er un graphique radar** comparant les 4 mÃ©triques par pipeline
4. **Identifier** quelle stratÃ©gie fonctionne le mieux sur votre type de documents

---

## ğŸ® Mode API pour IntÃ©gration

### DÃ©marrer l'API
```bash
uvicorn rag_chunk_lab.api:app --host 0.0.0.0 --port 8000
```

### Tester les 3 pipelines
```bash
# RÃ©ponse extractive (rapide)
curl "http://localhost:8000/ask?doc_id=test&question=Quel dÃ©lai de prescription ?" | jq .

# RÃ©ponse LLM (plus prÃ©cise)
curl "http://localhost:8000/ask?doc_id=test&question=Quel dÃ©lai de prescription ?&use_llm=true" | jq .
```

---

## âš™ï¸ Configuration AvancÃ©e

### Options de GÃ©nÃ©ration Ground Truth
```bash
python3 generate_ground_truth.py \
  --folder documents/juridique \
  --model llama3:8b \                    # ModÃ¨le plus performant
  --questions-per-doc 20 \               # Plus de questions
  --min-length 300 \                     # Textes plus longs
  --max-length 600 \                     # Limite plus haute
  --output dataset_juridique.jsonl       # Nom personnalisÃ©
```

### ParamÃ¨tres de Chunking
Modifiez dans `config.py` :
```python
DEFAULTS.fixed_size_tokens = 600        # Chunks plus grands
DEFAULTS.sliding_window = 500           # FenÃªtre plus large
DEFAULTS.top_k = 10                     # Plus de contexte
```

---

## ğŸ¯ Cas d'Usage Typiques

### 1. Documents Juridiques
- **Structure-aware** souvent meilleur (respecte articles/sections)
- Ground truth avec questions prÃ©cises sur procÃ©dures

### 2. Documentation Technique
- **Fixed chunks** bon compromis vitesse/qualitÃ©
- Questions sur API, configurations, troubleshooting

### 3. Rapports d'Analyse
- **Sliding window** capture mieux les relations entre sections
- Questions sur tendances, conclusions, recommandations

---

## ğŸ”§ Structure du Projet

```
rag_chunk_lab/
â”œâ”€â”€ cli.py                 # Interface ligne de commande
â”œâ”€â”€ api.py                 # API FastAPI
â”œâ”€â”€ chunkers.py            # 3 stratÃ©gies de chunking
â”œâ”€â”€ indexing.py            # Index TF-IDF + mÃ©tadonnÃ©es
â”œâ”€â”€ retrieval.py           # Recherche de candidats
â”œâ”€â”€ generation.py          # GÃ©nÃ©ration de rÃ©ponses
â”œâ”€â”€ evaluation.py          # MÃ©triques RAGAS
â”œâ”€â”€ ground_truth_generator.py  # GÃ©nÃ©ration auto de datasets
â””â”€â”€ utils.py               # Utilitaires PDF/texte

generate_ground_truth.py   # Script standalone
data/                      # Index et chunks par document
exports/                   # RÃ©sultats CSV d'Ã©valuation
```

---

## ğŸ’¡ Tips d'Optimisation

### Pour AmÃ©liorer les Performances
1. **Tester plusieurs tailles** de chunks (300, 500, 800 tokens)
2. **Ajuster l'overlap** selon le type de document (80-200 tokens)
3. **Utiliser structure-aware** sur documents bien structurÃ©s
4. **GÃ©nÃ©rer plus de questions** pour une Ã©valuation robuste (20+ par doc)

### Pour l'Analyse
1. **CrÃ©er des graphiques radar** dans Excel (4 mÃ©triques Ã— 3 pipelines)
2. **Segmenter par type de question** (procÃ©durale, factuelle, analytique)
3. **Comparer avec/sans LLM** pour voir l'impact de la gÃ©nÃ©ration
4. **Tester sur plusieurs documents** du mÃªme domaine

---

## ğŸ†˜ RÃ©solution de ProblÃ¨mes

### Ollama ne dÃ©marre pas
```bash
# VÃ©rifier si Ollama est installÃ©
ollama --version

# DÃ©marrer le service
ollama serve

# VÃ©rifier les modÃ¨les installÃ©s
ollama list
```

### Erreurs Azure OpenAI
```bash
# VÃ©rifier les variables d'environnement
echo $AZURE_OPENAI_API_KEY
echo $AZURE_OPENAI_ENDPOINT

# Tester la connexion
curl -H "api-key: $AZURE_OPENAI_API_KEY" "$AZURE_OPENAI_ENDPOINT/openai/deployments?api-version=2024-02-15-preview"
```

### Erreurs RAGAS "IndexError"
- **Cause** : Contextes vides ou rÃ©ponses manquantes
- **Solution** : Le code filtre automatiquement les entrÃ©es problÃ©matiques
- **Debug** : VÃ©rifier les logs pour voir combien d'entrÃ©es valides restent

---

## ğŸ‰ PrÃªt Ã  Optimiser Votre RAG !

1. **CrÃ©ez votre dataset** avec `generate_ground_truth.py`
2. **Comparez les stratÃ©gies** avec `evaluate --ragas`
3. **Analysez dans Excel** les fichiers CSV gÃ©nÃ©rÃ©s
4. **Choisissez la meilleure stratÃ©gie** pour votre cas d'usage
5. **IntÃ©grez via l'API** dans votre application

**RÃ©sultat** : Un pipeline RAG optimisÃ© spÃ©cifiquement pour vos documents ! ğŸš€