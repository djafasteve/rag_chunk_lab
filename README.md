# RAG Chunk Lab ‚Äî Laboratoire de Chunking pour RAG

Un outil complet pour **tester et comparer diff√©rentes strat√©gies de chunking** dans vos pipelines RAG. Id√©al pour optimiser la performance sur des documents juridiques, techniques ou r√©glementaires.

## üéØ Qu'est-ce que √ßa fait ?

**RAG Chunk Lab** teste automatiquement **3 strat√©gies de chunking** sur vos documents :

1. **üìÑ Fixed Chunks** ‚Üí D√©coupage fixe (500 tokens + overlap 80)
2. **üóÇÔ∏è Structure-Aware** ‚Üí Respecte les titres et sections (Article, Chapitre...)
3. **üîÑ Sliding Window** ‚Üí Fen√™tres glissantes (400 tokens, stride 200)

Pour chaque question, vous obtenez :
- ‚úÖ Une r√©ponse par strat√©gie (extractive ou LLM)
- üìç Les sources exactes (page, section, snippet)
- üìä Export CSV pour analyse dans Excel
- ü§ñ √âvaluation automatique avec m√©triques RAGAS

---

## üöÄ Installation et Premier Test

### √âtape 1 : Environnement
```bash
git clone <votre-repo>
cd rag_chunk_lab
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Pour le pipeline s√©mantique local (optionnel mais recommand√©)
pip install sentence-transformers

# Pour Azure OpenAI (embeddings cloud de qualit√© professionnelle)
pip install openai
export AZURE_OPENAI_API_KEY="votre-cl√©"
export AZURE_OPENAI_ENDPOINT="https://votre-resource.openai.azure.com"
export AZURE_OPENAI_DEPLOYMENT="text-embedding-ada-002"
export AZURE_OPENAI_EMBEDDING_DEPLOYMENT="text-embedding-ada-002"
export AZURE_OPENAI_API_VERSION="2024-02-15-preview"
```

### √âtape 2 : Ing√©rer vos documents

#### Option A : Un seul document
```bash
# Ing√©rer un document PDF/TXT/MD
python3 -m rag_chunk_lab.cli ingest --doc mon_document.pdf --doc-id test
```

#### Option B : Un dossier complet (üÜï Recommand√©)
```bash
# Ing√©rer tous les documents d'un dossier sous un seul doc-id
python3 -m rag_chunk_lab.cli ingest --doc mes_documents/ --doc-id ma_collection

# Support automatique : .pdf, .txt, .md
# Chaque document garde son nom de fichier source dans les m√©tadonn√©es
```

### √âtape 3 : Interroger votre collection

#### Option A : Analyse comparative des 5 strat√©gies
```bash
# Voir les 5 r√©ponses (fixed, structure, sliding, semantic, azure_semantic) pour analyser
python3 -m rag_chunk_lab.cli ask --doc-id ma_collection --question "Quel est le d√©lai de prescription ?"

# D√©sactiver les pipelines s√©mantiques si besoin
python3 -m rag_chunk_lab.cli ask --doc-id ma_collection --question "..." --no-semantic --no-azure-semantic
```

#### Option B : Chat IA avec r√©ponse synth√©tis√©e (üÜï Recommand√©)
```bash
# Obtenir une r√©ponse claire et contextuelle g√©n√©r√©e par l'IA (utilise Azure semantic par d√©faut)
python3 -m rag_chunk_lab.cli chat --doc-id ma_collection --question "Quel est le d√©lai de prescription ?"

# Avec mod√®le sp√©cialis√© pour l'expertise juridique
python3 -m rag_chunk_lab.cli chat \
  --doc-id ma_collection \
  --question "Quelles sont les sanctions en cas de r√©cidive ?" \
  --pipeline azure_semantic \
  --provider ollama \
  --model votre-modele-juridique \
  --top-k 5

# Ou avec le mod√®le par d√©faut
python3 -m rag_chunk_lab.cli chat \
  --doc-id ma_collection \
  --question "Quelles sont les sanctions en cas de r√©cidive ?" \
  --pipeline azure_semantic
```

**üß† Nouveaut√© : Pipelines S√©mantiques**

**üîπ Semantic (Local)** :
- üîç **Comprend le sens** : Trouve "sanctions" m√™me quand le texte dit "peines"
- üè† **Local** : Mod√®le fran√ßais `dangvantuan/sentence-camembert-large`
- üÜì **Gratuit** : Pas de co√ªt API

**‚òÅÔ∏è Azure Semantic (Cloud)** :
- üéØ **Qualit√© professionnelle** : Embeddings Azure OpenAI de derni√®re g√©n√©ration
- üìö **Optimis√© juridique** : Excellente compr√©hension des textes l√©gaux
- üåê **Multilingue** : Meilleure gestion fran√ßais/anglais
- ‚ö° **Pas de mod√®le lourd** : Traitement dans le cloud

**Avantages du mode chat :**
- üéØ **R√©ponse synth√©tis√©e** : L'IA combine et r√©sume les sources pertinentes
- üìö **Citations des sources** : R√©f√©rences aux documents et pages consult√©s
- üîç **Contextuel** : Utilise uniquement les informations de vos documents
- ‚ö° **Pr√™t √† l'emploi** : Fonctionne avec Ollama (local) ou Azure OpenAI

**R√©sultat** : 3 r√©ponses compar√©es + fichier `exports/test/sources_<timestamp>.csv`

| Commande   | ask                       | chat (üÜï)              |
  |------------|---------------------------|------------------------|
  | Sortie     | JSON brut des 3 pipelines | R√©ponse IA synth√©tis√©e |
  | Usage      | Analyse comparative       | Conversation naturelle |
  | Sources    | Chunks s√©par√©s            | Citations int√©gr√©es    |
  | Lisibilit√© | Technique                 | Grand public           |

  ü§ñ Fonctionnalit√©s cl√©s

  - ‚úÖ R√©ponse synth√©tis√©e : L'IA combine plusieurs sources et r√©sume
  - ‚úÖ Citations automatiques : R√©f√©rences aux documents et pages
  - ‚úÖ Contextuel : Utilise uniquement vos documents (pas d'hallucination)
  - ‚úÖ Configurable : Choix du pipeline, provider LLM, et nombre de sources
  - ‚úÖ Fallback robuste : Affiche les chunks m√™me si l'IA √©choue
  - ‚úÖ Support multimodal : Ollama (local/gratuit) et Azure OpenAI

---

## üß† Comprendre les 5 Strat√©gies de Recherche

### Pourquoi 5 approches diff√©rentes ?

Chaque m√©thode a ses forces selon le type de documents et de questions :

#### 1. **Fixed** (Chunks de taille fixe) ‚öñÔ∏è
- **Principe :** D√©coupe le texte en morceaux de taille r√©guli√®re
- **Id√©al pour :** Documents homog√®nes, recherches factuelles pr√©cises
- **Exemple :** "Quel est l'article 123 ?" dans un code juridique

#### 2. **Structure** (Conscient de la structure) üèóÔ∏è
- **Principe :** Respecte les titres, sections, paragraphes
- **Id√©al pour :** Documents bien structur√©s, recherches par section
- **Exemple :** "Que dit le chapitre sur les contrats ?" dans un manuel

#### 3. **Sliding** (Fen√™tre glissante) üîÑ
- **Principe :** Fen√™tres qui se chevauchent pour capturer les transitions
- **Id√©al pour :** Concepts qui s'√©tendent sur plusieurs paragraphes
- **Exemple :** "Comment fonctionne le processus de validation ?" (description longue)

#### 4. **Semantic** (S√©mantique Local) üß† **‚Üê Nouveaut√© !**
- **Principe :** Comprend le **sens** des mots avec un mod√®le IA local
- **Id√©al pour :** Questions en langage naturel, usage gratuit
- **Mod√®le :** `dangvantuan/sentence-camembert-large` (fran√ßais)

#### 5. **Azure Semantic** (S√©mantique Cloud) ‚òÅÔ∏è **‚Üê Premium !**
- **Principe :** Comprend le **sens** avec Azure OpenAI embeddings
- **Id√©al pour :** Documents juridiques, qualit√© maximale
- **Exemples magiques (communs aux 2 s√©mantiques) :**
  - Question: "sanctions" ‚Üí Trouve: "peines", "condamnations", "punitions"
  - Question: "d√©lai" ‚Üí Trouve: "dur√©e", "terme", "p√©riode"
  - Question: "interdit" ‚Üí Trouve: "prohib√©", "d√©fendu", "ill√©gal"

### üéØ Conseil Pratique

```bash
# 1. Commencez par tester les 5 approches
python3 -m rag_chunk_lab.cli ask --doc-id votre_doc --question "votre question"

# 2. Pour l'usage quotidien, privil√©giez Azure semantic (si configur√©)
python3 -m rag_chunk_lab.cli chat --doc-id votre_doc --question "votre question" --pipeline azure_semantic

# 3. Sinon, utilisez le s√©mantique local
python3 -m rag_chunk_lab.cli chat --doc-id votre_doc --question "votre question" --pipeline semantic

# 4. Avec un mod√®le sp√©cialis√© pour votre domaine d'expertise
python3 -m rag_chunk_lab.cli chat \
  --doc-id votre_doc \
  --question "votre question" \
  --model votre-modele-specialise
```

### ü§ñ Mod√®les LLM Recommand√©s

#### **Pour Documents Juridiques :**
```bash
# Mod√®le par d√©faut (g√©n√©raliste)
--model mistral:7b

# Mod√®les sp√©cialis√©s juridiques (si disponibles dans votre Ollama)
--model llama3:8b  # Meilleure compr√©hension contextuelle
--model "hf.co/MaziyarPanahi/calme-2.3-legalkit-8b-GGUF:Q8_0"  # Mod√®le juridique fran√ßais sp√©cialis√©
--model llama3.2:latest  # Compact et efficace
--model codellama:13b  # Si documents contiennent du code/r√©glementation
```

#### **Pour Documents Techniques :**
```bash
--model codellama:7b  # Sp√©cialis√© code et documentation technique
--model llama3:8b     # Bon compromis qualit√©/vitesse
```

#### **Configuration Permanente :**
Pour √©viter de r√©p√©ter `--model` √† chaque fois, modifiez dans `config.py` :
```python
# Pour usage juridique quotidien
DEFAULTS.default_model = "hf.co/MaziyarPanahi/calme-2.3-legalkit-8b-GGUF:Q8_0"

# Ou pour usage g√©n√©raliste rapide
DEFAULTS.default_model = "llama3.2:latest"
```

#### **Exemples Pratiques avec Mod√®le Juridique :**
```bash
# Question juridique avec mod√®le sp√©cialis√©
python3 -m rag_chunk_lab.cli chat \
  --doc-id codes_juridiques \
  --question "Quelles sont les conditions de la l√©gitime d√©fense ?" \
  --pipeline azure_semantic \
  --model "hf.co/MaziyarPanahi/calme-2.3-legalkit-8b-GGUF:Q8_0"

# Comparaison rapide avec mod√®le g√©n√©ral
python3 -m rag_chunk_lab.cli chat \
  --doc-id codes_juridiques \
  --question "Quelles sont les conditions de la l√©gitime d√©fense ?" \
  --pipeline azure_semantic \
  --model mistral:7b
```

---

## ü§ñ Cr√©er un Dataset de Test Automatiquement

### Pourquoi ?
Au lieu de cr√©er manuellement des questions/r√©ponses, **g√©n√©rez automatiquement un dataset d'expert** √† partir de vos documents !

### Comment faire ?

#### Option A : Avec Ollama (Local, Gratuit)
```bash
# 1. Installer et d√©marrer Ollama
ollama serve
ollama pull mistral:7b

# 2. G√©n√©rer 10 questions expertes par document
python3 generate_ground_truth.py --folder documents/mes_docs --questions-per-doc 10
```

#### Option B : Avec Azure OpenAI (Plus performant)
```bash
# 1. Configurer Azure OpenAI
export AZURE_OPENAI_API_KEY="votre-cl√©"
export AZURE_OPENAI_ENDPOINT="https://votre-resource.openai.azure.com"
export AZURE_OPENAI_DEPLOYMENT="gpt-4o-mini"
export AZURE_OPENAI_API_VERSION="2024-02-15-preview"

# 2. G√©n√©rer le dataset
python3 generate_ground_truth.py --folder documents/mes_docs --llm-provider azure
```

### R√©sultat
Fichier `mes_docs_ground_truth.jsonl` avec format :
```json
{
  "question": "Quel est le d√©lai de prescription pour les contraventions de 5√®me classe ?",
  "answer": "Le d√©lai de prescription de l'action publique des contraventions de la cinqui√®me classe est de trois ans r√©volus...",
  "source_document": "code_penal.pdf",
  "page": 15,
  "doc_section": "TITRE PREMIER",
  "generated_by": "ollama:mistral:7b"
}
```

---

## üìä √âvaluation et Comparaison des Strat√©gies

### √âvaluation Automatique avec RAGAS

Une fois votre dataset cr√©√©, comparez les 3 strat√©gies de chunking :

```bash
# √âvaluer les 3 pipelines avec m√©triques d'expert
python3 -m rag_chunk_lab.cli evaluate \
  --doc-id test \
  --ground-truth mes_docs_ground_truth.jsonl \
  --ragas \
  --use-llm
```

**Pendant l'ex√©cution, vous verrez :**
```
üîÑ Collecting answers from 3 pipelines for 10 questions...
üìä Processing pipelines: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:30<00:00]
‚úÖ Answer collection completed!

üéØ Starting RAGAS evaluation...
üîÑ Starting RAGAS evaluation for 3 pipelines with 10 questions...
üìä Metrics: answer_relevancy, faithfulness, context_precision, context_recall

üîç Evaluating fixed pipeline...
  üìù Running RAGAS metrics for 'fixed' pipeline...
  ‚úÖ fixed: answer_relevancy: 0.847, faithfulness: 0.923

‚ú® RAGAS evaluation completed!
üìã Summary:
  fixed: avg=0.856
  structure: avg=0.891  ‚Üê Meilleur !
  sliding: avg=0.834

üíæ Exporting results...
```

### Fichiers d'Analyse G√©n√©r√©s

Apr√®s √©valuation, vous trouvez dans `exports/test/` :

1. **`ragas_summary.csv`** ‚Üí Tableau de bord des moyennes
   ```
   pipeline,answer_relevancy,faithfulness,context_precision,context_recall
   fixed,0.847,0.923,0.756,0.834
   structure,0.891,0.945,0.812,0.867  ‚Üê La structure-aware gagne !
   sliding,0.834,0.898,0.743,0.801
   ```

2. **`ragas_per_question.csv`** ‚Üí D√©tail par question
   - Parfait pour identifier les questions probl√©matiques
   - Id√©al pour tableaux crois√©s dynamiques Excel

### Analyse dans Excel

1. **Ouvrir `ragas_summary.csv`** ‚Üí Vue d'ensemble rapide
2. **Ouvrir `ragas_per_question.csv`** ‚Üí Analyser les d√©tails
3. **Cr√©er un graphique radar** comparant les 4 m√©triques par pipeline
4. **Identifier** quelle strat√©gie fonctionne le mieux sur votre type de documents

---

## üí° Exemples Pratiques

### Cas d'Usage Typiques

#### üìö Collection de Documentation Technique
```bash
# Dossier avec manuels PDF, guides TXT, et docs Markdown
python3 -m rag_chunk_lab.cli ingest --doc documentation_produit/ --doc-id docs_techniques

# Questions: "Comment configurer SSL?", "Quels sont les pr√©requis?"
python3 -m rag_chunk_lab.cli chat --doc-id docs_techniques --question "Comment configurer SSL?" --pipeline semantic
```

#### ‚öñÔ∏è Corpus Juridique
```bash
# Dossier avec codes, jurisprudences, circulaires
python3 -m rag_chunk_lab.cli ingest --doc corpus_juridique/ --doc-id droit_penal

# Questions: "Quel est le d√©lai de prescription?", "Quelles sont les circonstances aggravantes?"
python3 -m rag_chunk_lab.cli chat --doc-id droit_penal --question "Quelles sont les circonstances aggravantes?" --pipeline semantic
```

#### üè¢ Base de Connaissances Entreprise
```bash
# Proc√©dures, politiques, manuels RH
python3 -m rag_chunk_lab.cli ingest --doc knowledge_base/ --doc-id entreprise

# Questions: "Quelle est la politique de t√©l√©travail?", "Comment demander un cong√©?"
python3 -m rag_chunk_lab.cli chat --doc-id entreprise --question "Quelle est la politique de t√©l√©travail?" --pipeline semantic
```

### Workflow Complet Recommand√©

```bash
# 1. Ing√©rer votre collection de documents
python3 -m rag_chunk_lab.cli ingest --doc mes_documents/ --doc-id ma_collection

# 2. G√©n√©rer automatiquement un dataset de test
python3 generate_ground_truth.py --folder mes_documents --questions-per-doc 5

# 3. √âvaluer et comparer les 3 strat√©gies
python3 -m rag_chunk_lab.cli evaluate \
  --doc-id ma_collection \
  --ground-truth mes_documents_ground_truth.jsonl \
  --ragas --use-llm

# 4. Analyser les r√©sultats dans Excel
# Ouvrir exports/ma_collection/ragas_summary.csv

# 5. Utiliser la strat√©gie s√©mantique pour un usage quotidien optimal
python3 -m rag_chunk_lab.cli chat --doc-id ma_collection --question "Votre question" --pipeline semantic
```

---

## üéÆ Mode API pour Int√©gration

### D√©marrer l'API
```bash
uvicorn rag_chunk_lab.api:app --host 0.0.0.0 --port 8000
```

### Tester les 3 pipelines
```bash
# R√©ponse extractive (rapide)
curl "http://localhost:8000/ask?doc_id=test&question=Quel d√©lai de prescription ?" | jq .

# R√©ponse LLM (plus pr√©cise)
curl "http://localhost:8000/ask?doc_id=test&question=Quel d√©lai de prescription ?&use_llm=true" | jq .
```

---

## ‚öôÔ∏è Configuration Avanc√©e

### Options de G√©n√©ration Ground Truth
```bash
python3 generate_ground_truth.py \
  --folder documents/juridique \
  --model llama3:8b \                    # Mod√®le plus performant
  --questions-per-doc 20 \               # Plus de questions
  --min-length 300 \                     # Textes plus longs
  --max-length 600 \                     # Limite plus haute
  --output dataset_juridique.jsonl       # Nom personnalis√©
```

### Param√®tres de Chunking
Modifiez dans `config.py` :
```python
DEFAULTS.fixed_size_tokens = 600        # Chunks plus grands
DEFAULTS.sliding_window = 500           # Fen√™tre plus large
DEFAULTS.top_k = 10                     # Plus de contexte
```

---

## üéØ Cas d'Usage Typiques

### 1. Documents Juridiques
- **Structure-aware** souvent meilleur (respecte articles/sections)
- Ground truth avec questions pr√©cises sur proc√©dures

### 2. Documentation Technique
- **Fixed chunks** bon compromis vitesse/qualit√©
- Questions sur API, configurations, troubleshooting

### 3. Rapports d'Analyse
- **Sliding window** capture mieux les relations entre sections
- Questions sur tendances, conclusions, recommandations

---

## üîß Structure du Projet

```
rag_chunk_lab/
‚îú‚îÄ‚îÄ cli.py                 # Interface ligne de commande
‚îú‚îÄ‚îÄ api.py                 # API FastAPI
‚îú‚îÄ‚îÄ chunkers.py            # 3 strat√©gies de chunking
‚îú‚îÄ‚îÄ indexing.py            # Index TF-IDF + m√©tadonn√©es
‚îú‚îÄ‚îÄ retrieval.py           # Recherche de candidats
‚îú‚îÄ‚îÄ generation.py          # G√©n√©ration de r√©ponses
‚îú‚îÄ‚îÄ evaluation.py          # M√©triques RAGAS
‚îú‚îÄ‚îÄ ground_truth_generator.py  # G√©n√©ration auto de datasets
‚îî‚îÄ‚îÄ utils.py               # Utilitaires PDF/texte

generate_ground_truth.py   # Script standalone
data/                      # Index et chunks par document
exports/                   # R√©sultats CSV d'√©valuation
```

---

## üí° Tips d'Optimisation

### Pour Am√©liorer les Performances
1. **Tester plusieurs tailles** de chunks (300, 500, 800 tokens)
2. **Ajuster l'overlap** selon le type de document (80-200 tokens)
3. **Utiliser structure-aware** sur documents bien structur√©s
4. **G√©n√©rer plus de questions** pour une √©valuation robuste (20+ par doc)

### Pour l'Analyse
1. **Cr√©er des graphiques radar** dans Excel (4 m√©triques √ó 3 pipelines)
2. **Segmenter par type de question** (proc√©durale, factuelle, analytique)
3. **Comparer avec/sans LLM** pour voir l'impact de la g√©n√©ration
4. **Tester sur plusieurs documents** du m√™me domaine

---

## üÜò R√©solution de Probl√®mes

### Ollama ne d√©marre pas
```bash
# V√©rifier si Ollama est install√©
ollama --version

# D√©marrer le service
ollama serve

# V√©rifier les mod√®les install√©s
ollama list
```

### Erreurs Azure OpenAI
```bash
# V√©rifier les variables d'environnement
echo $AZURE_OPENAI_API_KEY
echo $AZURE_OPENAI_ENDPOINT

# Tester la connexion
curl -H "api-key: $AZURE_OPENAI_API_KEY" "$AZURE_OPENAI_ENDPOINT/openai/deployments?api-version=2024-02-15-preview"
```

### Erreurs RAGAS "IndexError"
- **Cause** : Contextes vides ou r√©ponses manquantes
- **Solution** : Le code filtre automatiquement les entr√©es probl√©matiques
- **Debug** : V√©rifier les logs pour voir combien d'entr√©es valides restent

---

## üéâ Pr√™t √† Optimiser Votre RAG !

1. **Cr√©ez votre dataset** avec `generate_ground_truth.py`
2. **Comparez les strat√©gies** avec `evaluate --ragas`
3. **Analysez dans Excel** les fichiers CSV g√©n√©r√©s
4. **Choisissez la meilleure strat√©gie** pour votre cas d'usage
5. **Int√©grez via l'API** dans votre application

**R√©sultat** : Un pipeline RAG optimis√© sp√©cifiquement pour vos documents ! üöÄ